# Evaluate the performance of your application in the Azure AI Foundry

This exercise will guide you through exploring Manual and Automatic evaluations to assess and compare the performance of your AI applications using the Azure AI Foundry portal. 

## Prerequisites
- Create an AI hub and project in the Azure AI Foundry
- Deploy a GPT model

## Manually evaluate your application in the Azure AI Foundry portal


1. Navigate to the **Models + endpoints** page under the **My assets** section using the left menu.
2. Select the **gpt-4O-mini** model and click on **Open in playground**.
3. In the **Give the model instructions and context** text box, replace the content with the following:
   
   ```
   **Objective**: You are an intelligent and helpful AI assistant specializing in answering questions about the Northwind Standard Benefits Plan. Your knowledge is based on the official Northwind Standard Benefits Details document, which includes information on medical, vision, and dental coverage, in-network vs. out-of-network providers, costs (premiums, deductibles, copayments, coinsurance, and out-of-pocket maximums), covered services, exclusions, and additional benefits.

   **Instructions:**  
   1. **Be Accurate & Grounded:**  
   - All responses must be based strictly on the provided document.   
   - Do not speculate, provide legal advice, or make assumptions.  
   - If the document does not contain an answer, politely indicate that.  

   2. **Be Concise & Clear:**  
   - Provide direct, easy-to-understand answers without unnecessary complexity.  
   - When necessary, summarize relevant sections while maintaining completeness.  

   3. **Context Awareness:**  
   - If a user asks about costs (e.g., premiums, deductibles, copays), ensure you clarify which category applies.  
   - If a user asks about in-network vs. out-of-network, highlight the cost differences and coverage implications.  

   4. **Handling Special Cases:**  
   - If a user asks about **exceptions or exclusions**, refer to the relevant section in the document.  
   - If a user asks for **tips on managing healthcare costs**, provide the recommendations listed in the document.  
   - If a user asks for **provider networks**, suggest they verify with Northwind Health’s website or customer service.  

   5. **Cite Document Sections When Needed:**  
   - If a user requests specific details, refer to the relevant section (e.g., “As outlined in the ‘Covered Services’ section…”).  
   - Avoid generic or overly broad answers—stick to the document content.  

   6. **Unsupported Questions:**  
   - If a user asks about services not covered in the document (e.g., specific hospital partnerships, claim processing times), inform them that the information is unavailable.  

   ```

6. Select **Apply changes**.

7. In the chat (history) window, enter the query: "What can you do?" to verify that the language model is behaving as expected.


To manually review model responses based on test data:

1. In the **Chat playground**, select the **Evaluate** dropdown from the top bar, and choose **Manual evaluation**.
2. Change the **System message** to the same message as used above.
3. In the **Manual evaluation result** section, add the following five questions as separate **Inputs**:

   - Input: What is the annual deductible for the Northwind Standard plan?
   - expected_answer: The deductible for the Northwind Standard plan is $2,000 per person, per year.

   - Input: What is the copayment in Northwind Standard plan for a specialist visit?
   - expected_answer: The copayment for a specialist visit under the Northwind Standard plan is $50

   - Input: Are preventive care services covered?
   - expected_answer: Yes, preventive care services are covered at 100% under the Northwind Standard plan
   
4. Select **Run** from the top bar to generate outputs for all questions.
5. Manually review the outputs for each question by selecting the thumbs up or down icon at the bottom right of a response. Ensure you include at least one thumbs up and one thumbs down response in your ratings.
6. Select **Save results** from the top bar. Enter `manual_evaluation_results` as the name for the results.
7. Navigate to **Evaluation** using the left menu.
8. Select the **Manual evaluations** tab to find the manual evaluations you just saved.

## Evaluate your Application with Automatic Evaluations

To evaluate a chat flow using built-in metrics:

1. Select the **Automated evaluations** tab and create a **New evaluation** with the following settings:
   - **What do you want to evaluate?**: Dataset
   - **Evaluation name**: *Enter a unique name*
2. Select **Next**
3. For **Select the data you want to evaluate**: Add your dataset
     - Use the validation dataset located at `data\NW_HealthPlan_Standard_Dataset.jsonl`,and upload it to the UI.
4. Select **Next**
5. Configure the following in Select metrics:
   - **AI quality(AI Assisted):**  Groundedness, Relevance,Coherence, Similarity 
   - **Connection**: *Your AI Services connection*
   - **Deployment name/Model**: *Your deployed GPT-4O-mini model*
   - **Risk and safety metrics(AI Assisted):**  self-harm-related content, Hateful and unfair content, voilent content
   - **context**: Select **${data.context}** as the data source
   - **response**: Select **${data.expected_answer}** as the data source
   - **query**: Select **${data.query}** as the data source
   - **ground_truth**: Select **${data.ground_truth}** as the data source
6. Select **Next**, review your data, and **Submit** the new evaluation.
7. Wait for the evaluations to complete, refreshing if necessary.
8. Select the evaluation run you just created.
9. Explore the **Metric dashboard** and **Detailed metrics result**.


